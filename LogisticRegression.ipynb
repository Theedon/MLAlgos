{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFPn7bqH/WwvDk9wvWawfN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theedon/MLAlgos/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvwxZEe58dhg",
        "outputId": "ddec1f72-f67e-47a8-8f1e-5e7976feee33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n",
            "['/usr/local/lib/python3.10/dist-packages/numpy']\n"
          ]
        }
      ],
      "source": [
        "# Logistic regression class with forward and back prop\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "print(np.__version__)\n",
        "print(np.__path__)\n",
        "\n",
        "class LogisticRegression():\n",
        "  def __init__(self, lr = 0.001, num_iterations = 1000):\n",
        "    self.lr = lr\n",
        "    self.num_iterations = num_iterations\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "# activation method\n",
        "  def activate(self, Z, func: str = \"sigmoid\"):\n",
        "    if func == \"relu\":\n",
        "      A =  np.maximum(0, Z)\n",
        "    elif func == \"sigmoid\":\n",
        "      Z = np.clip(Z, -500, 500)\n",
        "      A =  1 / (1 + np.exp(-Z))\n",
        "    elif func == \"tanh\":\n",
        "      # A =  (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
        "      A =  np.tanh(Z)\n",
        "    elif func == \"none\":\n",
        "      A = Z\n",
        "\n",
        "    return A\n",
        "\n",
        "# fitting step\n",
        "  def fit(self, X, y):\n",
        "    m = X.shape[0]\n",
        "    X = X.T\n",
        "    self.weights = np.zeros(X.shape[0]).reshape(-1, 1)\n",
        "    self.bias = 0\n",
        "\n",
        "    tic = time.time()\n",
        "    for i in range(self.num_iterations):\n",
        "      # forward pass\n",
        "      Z = np.dot(self.weights.T, X) + self.bias\n",
        "      A = self.activate(Z, func=\"sigmoid\")\n",
        "\n",
        "      # calculate cost\n",
        "      cost = -np.sum(y * np.log(A) + (1-y) * np.log(1-A))/m\n",
        "\n",
        "      # compute gradients\n",
        "      dw = np.dot(X, (A-y).T)/m\n",
        "      db = np.sum(A - y)/m\n",
        "\n",
        "      # back propagation\n",
        "      self.weights = self.weights - self.lr * dw\n",
        "      self.bias = self.bias - self.lr * db\n",
        "\n",
        "      if i % int(self.num_iterations/20) == 0:\n",
        "        print(f\"cost after iteration {i} is {cost}\")\n",
        "    toc = time.time()\n",
        "    print(f\"time taken to train is {toc - tic} seconds\")\n",
        "\n",
        "  def predict(self, X):\n",
        "    Z = np.dot(X, self.weights) + self.bias\n",
        "    A = self.activate(Z, func=\"sigmoid\")\n",
        "    pred = (A > 0.5).astype(int)\n",
        "    return pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate sample data\n",
        "\n",
        "def generate_sample_data(num_examples: int|None = 50, num_features: int|None = 2):\n",
        "  np.random.seed(42)\n",
        "\n",
        "  m_each = int(num_examples or 50/2)\n",
        "  X0 = np.random.randn(m_each, 2) - 1\n",
        "  y0 = np.zeros(m_each)\n",
        "\n",
        "  X1 = np.random.randn(m_each, 2) + 1\n",
        "  y1 = np.ones(m_each)\n",
        "\n",
        "  X = np.vstack((X0, X1))\n",
        "  y = np.hstack((y0, y1))\n",
        "\n",
        "\n",
        "  indices = np.arange(X.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  X = X[indices]\n",
        "  y = y[indices]\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "Ob2_l8g78yRu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate sample data for training\n",
        "\n",
        "num_examples = 10000\n",
        "num_features = 40\n",
        "\n",
        "X_train, y_train = generate_sample_data(num_examples = num_examples, num_features=num_features)\n",
        "X_test, y_test = generate_sample_data(num_examples = int(num_examples/2), num_features=num_features)"
      ],
      "metadata": {
        "id": "fKulnvmG823r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(lr=0.001, num_iterations = 50000)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YBkDQnI8ov0",
        "outputId": "d6cf332a-00ee-4e87-b021-ebf4536d706f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost after iteration 0 is 0.6931471805599453\n",
            "cost after iteration 2500 is 0.31549702167745136\n",
            "cost after iteration 5000 is 0.25719173365199677\n",
            "cost after iteration 7500 is 0.23428722398365212\n",
            "cost after iteration 10000 is 0.22225514207399197\n",
            "cost after iteration 12500 is 0.21497165472669238\n",
            "cost after iteration 15000 is 0.21017478669147358\n",
            "cost after iteration 17500 is 0.20683384568564292\n",
            "cost after iteration 20000 is 0.2044124659039283\n",
            "cost after iteration 22500 is 0.2026045933803124\n",
            "cost after iteration 25000 is 0.20122334889876003\n",
            "cost after iteration 27500 is 0.20014853778691133\n",
            "cost after iteration 30000 is 0.1992996130062573\n",
            "cost after iteration 32500 is 0.19862077096402764\n",
            "cost after iteration 35000 is 0.1980722741115678\n",
            "cost after iteration 37500 is 0.19762516643549186\n",
            "cost after iteration 40000 is 0.19725793161351476\n",
            "cost after iteration 42500 is 0.19695431087865486\n",
            "cost after iteration 45000 is 0.19670183831191496\n",
            "cost after iteration 47500 is 0.19649083384066862\n",
            "time taken to train is 134.71822786331177 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on test set\n",
        "a = model.predict(X_test)"
      ],
      "metadata": {
        "id": "XXmE0oR99ePR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy on test set\n",
        "\n",
        "c = 0\n",
        "for i in range(a.shape[1]):\n",
        "  if int(a[0, i]) == int(y_test[i]):\n",
        "    c += 1\n",
        "\n",
        "\n",
        "accuracy = (c / a.shape[1]) * 100  # Calculate accuracy\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eaokhofp9gN6",
        "outputId": "26c5e374-cbbe-4a53-dfd9-2c1951fad191"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    }
  ]
}